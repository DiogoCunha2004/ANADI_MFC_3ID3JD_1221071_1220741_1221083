{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f27ae2",
   "metadata": {},
   "source": [
    "# Váriaveis Repetidas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01290e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "alpha = 0.05\n",
    "test_size = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb8a91e",
   "metadata": {},
   "source": [
    "# 4-3-3.ipynb\n",
    "\n",
    "# Obtenha a média e o desvio padrão da Accuracy; Sensitivity; Specificity e F1 do atributo RespDisease com os modelos obtidos na alínea anterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a597f7a9",
   "metadata": {},
   "source": [
    "# Leitura de Dados e filtração de Dados de Alinea anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d39cfff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comando read_csv para tal.\n",
    "dados = pd.read_csv('../../dados/AIRPOL_data.csv', delimiter=\";\", header=0, decimal=',')\n",
    "dados = dados.drop(columns=['Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15'])\n",
    "\n",
    "\n",
    "goalAttrib = 'RespDisease'\n",
    "\n",
    "def calc_resp_disease(row):\n",
    "    respDiseases = ['Asthma', 'Chronic obstructive pulmonary disease']\n",
    "    if row['Outcome'] in respDiseases:  # Use 'Outcome' instead of 'Disease'\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "dados[goalAttrib] = dados.apply(calc_resp_disease, axis=1)\n",
    "dados \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f16aa42",
   "metadata": {},
   "source": [
    "# K- FOLD Cross Validation (linha anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f51f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(dados.columns[0:9])\n",
    "numericFeatures = features[4:]\n",
    "print(numericFeatures)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = dados[numericFeatures].drop(columns=[goalAttrib])\n",
    "y = dados[goalAttrib]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=test_size,random_state=random_state)\n",
    "\n",
    "print(\"Stratified division of goal attribute:\")\n",
    "print(y_train.value_counts(normalize=True).mul(100).round(1).astype(str)+'%')\n",
    "print(y_test.value_counts(normalize=True).mul(100).round(1).astype(str)+'%')\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69b76cb",
   "metadata": {},
   "source": [
    "## Definição dos Modelos K-NEIGHBOURS,SVM,REGRESSION THREE,NEURAL NETWORK With ITS CLASSIFIERES com cada um dos seus construtores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309131d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "models.append(('dtr', DecisionTreeClassifier(  # Decision Tree Classifier\n",
    "    max_depth=10,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=random_state\n",
    ")))\n",
    "\n",
    "models.append(('net', MLPClassifier(\n",
    "    hidden_layer_sizes=(5, 6),\n",
    "    activation='tanh',\n",
    "    solver='lbfgs',\n",
    "    max_iter=1000,\n",
    "    learning_rate='adaptive',\n",
    "    early_stopping=True,\n",
    "    random_state=random_state\n",
    ")))\n",
    "\n",
    "models.append(('knn', KNeighborsClassifier(n_neighbors=21)))\n",
    "models.append(('svm', make_pipeline(StandardScaler(), SVC(kernel='rbf'))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b26378",
   "metadata": {},
   "source": [
    "# Definição das Varaiáveis  [Accuracy, Sensitivity, Specificity, F1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5182bf82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, confusion_matrix, f1_score\n",
    "\n",
    "# Funções para Sensitivity (recall) e Specificity\n",
    "def sensitivity(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, pos_label=1)\n",
    "\n",
    "def specificity(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn = cm[0, 0]\n",
    "    fp = cm[0, 1]\n",
    "    return tn / (tn + fp)\n",
    "    \n",
    "# Preparar dados (imputação de NaN)\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "#Scorer personalizados\n",
    "scorers = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'sensitivity': make_scorer(sensitivity),\n",
    "    'specificity': make_scorer(specificity),\n",
    "    'f1': make_scorer(f1_score)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b6317a",
   "metadata": {},
   "source": [
    "# Cross Validation para accucary , sensitivity, specificity e f1 usando os modelos definidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ceda8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize results dictionary\n",
    "results = {name: {metric: [] for metric in scorers} for name, _ in models}\n",
    "\n",
    "# Método usado para calcular as métricas\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "\n",
    "for name, model in models:\n",
    "    print(f'\\nTreinando modelo: {name}')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X_imputed, y)):\n",
    "        fold_start = time.time()\n",
    "        \n",
    "        X_train, X_test = X_imputed.iloc[train_idx], X_imputed.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        results[name]['accuracy'].append(accuracy_score(y_test, y_pred))\n",
    "        results[name]['sensitivity'].append(sensitivity(y_test, y_pred))\n",
    "        results[name]['specificity'].append(specificity(y_test, y_pred))\n",
    "        results[name]['f1'].append(f1_score(y_test, y_pred))\n",
    "        \n",
    "        fold_end = time.time()\n",
    "        print(f'  Fold {fold_idx + 1} em {fold_end - fold_start:.2f} segundos')\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f'Tempo total para {name}: {end_time - start_time:.2f} segundos')\n",
    "\n",
    "summary = []\n",
    "\n",
    "for model_name, metrics in results.items():\n",
    "    row = {'Model': model_name}\n",
    "    for metric_name, values in metrics.items():\n",
    "        row[f'{metric_name}_mean'] = np.mean(values)\n",
    "        row[f'{metric_name}_std'] = np.std(values)\n",
    "    summary.append(row)\n",
    "\n",
    "df_summary = pd.DataFrame(summary)\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e6f2c9",
   "metadata": {},
   "source": [
    "# 4-3-4  Verifique  se  existe  diferença  significativa  no  desempenho  dos  dois  melhores modelos obtidos anteriormente (use um nível de significância de 5%). Identifique o modelo que apresenta o melhor desempenho"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4aa16f7",
   "metadata": {},
   "source": [
    "#  (Hold out 80% 20%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53140574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "\n",
    "# Divide os dados em 80% treino e 20% teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y, test_size=0.20, stratify=y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f989d9d5",
   "metadata": {},
   "source": [
    "### 2 Melhores modelos Escolhidos\n",
    "\n",
    "1. **Decision Tree (DTR)**  \n",
    "2. **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "## Explicação \n",
    "## Melhor modelo DTR\n",
    "- **Maior F1-score** (`0.6577`), mostra excelente equilíbrio entre precisão e a sensibilidade.\n",
    "- **Maior sensibilidade** (`0.5124`), ou seja, o modelo consegue identificar corretamente mais casos positivos.\n",
    "- **Melhor acuracy** (`0.7966`)  \n",
    "- **specificity** (`0.9718`), o que confirma um desempenho forte e estável para além de isso tem um **baixo desvio padrão** (`0.0071`), indicando que o modelo é consistente em diferentes folds da validação cruzada.\n",
    "\n",
    "\n",
    "## Segundo melhor modelo:KNN\n",
    "- Foi o segundo melhor em **F1-score** (`0.4021`) e em **sensibilidade** (`0.2585`).\n",
    "- Apesar de inferior ao DTR, teve um desempenho consideravelmente melhor do que os modelos SVM e Net, que falharam em identificar casos positivos.\n",
    "- Também apresentou **specificity alta** (`0.9831`) e **boa estabilidade** nos resultados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da30cfee",
   "metadata": {},
   "source": [
    "# Uso de Decesion Tree para obter o melhor modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34af179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Treinando modelo: dtr\n",
      "  Fold 1 em 0.22 segundos\n",
      "  Fold 2 em 0.27 segundos\n",
      "  Fold 3 em 0.16 segundos\n",
      "  Fold 4 em 0.30 segundos\n",
      "  Fold 5 em 0.22 segundos\n",
      "Tempo total para dtr: 1.20 segundos\n",
      "\n",
      "Treinando modelo: net\n",
      "  Fold 1 em 0.58 segundos\n",
      "  Fold 2 em 0.49 segundos\n",
      "  Fold 3 em 0.27 segundos\n",
      "  Fold 4 em 0.38 segundos\n",
      "  Fold 5 em 0.46 segundos\n",
      "Tempo total para net: 2.21 segundos\n",
      "\n",
      "Treinando modelo: knn\n",
      "  Fold 1 em 1.68 segundos\n",
      "  Fold 2 em 1.38 segundos\n",
      "  Fold 3 em 1.47 segundos\n",
      "  Fold 4 em 1.07 segundos\n",
      "  Fold 5 em 1.05 segundos\n",
      "Tempo total para knn: 6.67 segundos\n",
      "\n",
      "Treinando modelo: svm\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e99022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que os dois melhores modelos sejam 'dtr' e 'knn'\n",
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Obter os F1-scores de cada modelo nos 5 folds da cross-validation\n",
    "f1_dtr = results['dtr']['F1']['Valores']  # Lista com os F1 do modelo DTR\n",
    "f1_knn = results['knn']['F1']['Valores']  # Lista com os F1 do modelo KNN\n",
    "\n",
    "# Aplicar teste t pareado\n",
    "stat, p_value = ttest_rel(f1_dtr, f1_knn)\n",
    "\n",
    "print(\"Resultado do teste t para comparação dos modelos DTR e KNN:\")\n",
    "print(f\"Estatística t: {stat:.4f}\")\n",
    "print(f\"Valor-p: {p_value:.4f}\")\n",
    "\n",
    "# Conclusão baseada no valor-p\n",
    "if p_value < 0.05:\n",
    "    print(\"\\nExiste diferença estatisticamente significativa entre os dois modelos.\")\n",
    "    melhor = 'dtr' if np.mean(f1_dtr) > np.mean(f1_knn) else 'knn'\n",
    "    print(f\" O modelo com melhor desempenho médio é: {melhor.upper()}\")\n",
    "else:\n",
    "    print(\"\\n Não existe diferença estatisticamente significativa entre os modelos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e748c23",
   "metadata": {},
   "source": [
    "# 4-3-5 Compare os resultados dos modelos. Discuta em detalhe qual o modelo que apresentou melhor e pior desempenho de acordo com os critérios: Accuracy; Sensitivity; Specificity e F1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310300f9",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Melhor Desempenho (DTR)\n",
    "  **Accuracy:** 0.7966 → maior taxa geral de acertos.\n",
    "  **Sensitivity:** 0.5124 → o único modelo que conseguiu identificar corretamente mais de 50% dos casos opositivos.\n",
    "  **Specificity:** 0.9718 → bom desempenho na identificação dos negativos.\n",
    "  **F1-score:** 0.6577 → o mais alto entre todos, indicando equilíbrio entre precisão e sensibilidade.\n",
    "  \n",
    "  **Conclusão 1:** O DTR é o **modelo mais completo e equilibrado**, com desempenho superior em todas as métricas mais relevantes.\n",
    "\n",
    "\n",
    "#### Pior Desempenho (Net)\n",
    "  **Accuracy:** 0.6187 → desempenho fraco no geral.\n",
    "  **Sensitivity:** 0.0000 → nenhum caso positivo.\n",
    "  **Specificity:** 1.0000 → classificou todos os negativos corretamente, mas ignorou completamente os positivos.\n",
    "  **F1-score:** 0.0000 → desempenho inaceitável para problemas que exigem detecção de positivos.\n",
    "  **Conclusão 2:** Apesar da especificidade perfeita, o modelo é **inútil para detectar casos relevantes**.\n",
    "\n",
    "\n",
    "###  Conclusão Geral\n",
    "O **modelo com melhor desempenho global** é o **Decision Tree (DTR)**, sendo o único com boas métricas em todos os critérios.\n",
    "O **pior modelo** foi o **Net**, que falhou completamente na detecção da classe positiva, mesmo apresentando accuracy e specificity aparentemente aceitáveis.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
